---
title: "11 --- Text as Data"
format: kjhslides-revealjs
engine: knitr
filters:
  - invert-h1
  - line-highlight
  - include-code-files
author:
  - name: Kieran Healy
date: last-modified
editor_options: 
  chunk_output_type: console
---


```{r}
#| label: "packages"
#| include: FALSE
library(flipbookr)
library(here)
library(tidyverse)
library(kjhslides)
```


```{r}
#| label: "setup"
#| include: FALSE

kjh_register_tenso()
kjh_set_knitr_opts()
kjh_set_slide_theme()







# Safe
```


---

![](img/10_domingos_climate.png)


---

![](img/10_domingos_climate_02.png)


---
 
![](img/10_us_pop_dens.png)


---


![](img/10_us_pct_black.png)

---

![](img/10_us_blackbelt_dotdensity.png)

---

![](img/10_dot_density_race_nyc.png)


---

![](img/10_detroit_8milerd.png)

---

![](img/10_brooklyn_holc_sm.png)

---

![](img/10_durham_holc.png)

---

![](img/10_durham_holc_modern.png)


---

![](img/10_durham_strava.png)


---

![](img/10_columbus_holc.png)

---

![](img/10_columbus_strava.png)


# [Text]{.fg-green} as [Data]{.fg-lblue}


## Load the packages, as always


```{r}
#| label: "07-iterating-on-data-2"
#| message: TRUE
library(here)      # manage file paths
library(socviz)    # data and some useful functions
library(tidyverse) # your friend and mine
```

## Specialty packages


```{r}
#| label: "07-iterating-on-data-3"
#| message: TRUE

#install.packages("tidytext")
#install.packages("gutenbergr")
#install.packages("janeaustenr")

library(tidytext)    # Tools for analyzing text
library(gutenbergr)  # Get books from Project Gutenberg
library(janeaustenr) # Pre-organized dataset of Jane Austen's novels
```

::: aside
This week's examples are mostly taken from Silge and Robinson (2017), _Text Mining with R_.  
:::



## Tidy text

```{r}
original_books <- austen_books() |>
  group_by(book) |>
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) |>
  ungroup()

tidy_books <- original_books |>
  unnest_tokens(word, text)

tidy_books
```

## "Stopwords"

- For many purposes (not always!) very common words like prepositions and articles are not interesting.

```{r}
data(stop_words)

stop_words

tidy_books <- tidy_books |>
  anti_join(stop_words)
```

## Stopwords removed

```{r}
tidy_books |>
  count(word, sort = TRUE) 
```


## Word frequency

```{r}
#| fig.width: 5
#| fig.height: 4
tidy_books |>
  count(word, sort = TRUE) |>
  filter(n > 600) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) + geom_col() +
  labs(y = NULL)
```

## `tf-idf`

Stands for "Term Frequency–Inverse Document Frequency"

The idea is to count the frequency of terms in a document, but decrease the weight of commonly used words and increase the weight for words that are not used very much in a corpus. 

## For example ...

```{r}
book_words <- austen_books() |>
  unnest_tokens(word, text) |>
  count(book, word, sort = TRUE)

total_words <- book_words |> 
  group_by(book) |> 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words
```

## For example ...


```{r}
#| fig.width: 24
#| fig.height: 4
ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, nrow = 2, scales = "free_y")
```

## Zipf's Law

"The frequency that a word appears is inversely proportional to its rank."

```{r}
freq_by_rank <- book_words |> 
  group_by(book) |> 
  mutate(rank = row_number(), 
         `term frequency` = n/total) |>
  ungroup()

freq_by_rank
```

## Zipf's Law

```{r}
#| fig.height: 6
#| fig.width: 8
freq_by_rank |> 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```


## n-grams

```{r}
austen_bigrams <- austen_books() |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  filter(!is.na(bigram))

austen_bigrams
```

## n-grams

```{r}
austen_bigrams |>
  count(bigram, sort = TRUE)
```

Stopwords again.

## n-grams

Split the columns

```{r}
bigrams_separated <- austen_bigrams |>
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered |> 
  count(word1, word2, sort = TRUE)

bigram_counts
```

## n-grams

```{r}
bigrams_united <- bigrams_filtered |>
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

Now we have common bigrams without stopwords.

## n-gram tf-idf

```{r}
bigram_tf_idf <- bigrams_united |>
  count(book, bigram) |>
  bind_tf_idf(bigram, book, n) |>
  arrange(desc(tf_idf))

bigram_tf_idf
```


## Plot them 

```{r}
out <- bigram_tf_idf |>
  arrange(desc(tf_idf)) |>
  group_by(book) |>
  slice_max(tf_idf, n = 10) |>
  ungroup() |>
  mutate(bigram = reorder(bigram, tf_idf)) |>
  ggplot(aes(tf_idf, bigram, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ book, nrow = 2, scales = "free") +
  labs(x = "tf-idf of bigram", y = NULL)
```


---

```{r}
#| fig.width: 15
#| fig.height: 6
print(out)
```

## Sentiment Analysis

```{r}
ulysses <- "http://aleph.gutenberg.org/4/3/0/4300/4300-0.txt"
raw_text <- tibble(text = readr::read_lines(ulysses))

raw_text
```


## Sentiment Analysis

```{r}
raw_text[74,]

raw_text[nrow(raw_text) - 360,]
```


## Sentiment Analysis

```{r}
full_text <- raw_text |> 
  mutate(line=row_number()) |>
  slice(-seq(n(), n() - 359)) |> # end
  slice(-seq(1:73)) |> # top
  unnest_tokens(word,text)
```


## Sentiment Analysis

```{r}
full_text[1:31,] |> 
  print(n = Inf)
```

## Sentiment Analysis 

```{r}
tail(full_text, n = 15)
```

## Sentiment Analysis

```{r}
full_text |>
  anti_join(stop_words) |>
  filter(! str_detect(word, "'")) |>
  filter(! str_detect(word, "’")) |>
  count(word, sort = TRUE) |>
  top_n(20) |>
  mutate(word=reorder(word, n))
```


## Sentiment Analysis

```{r}
sent <- full_text |> 
  inner_join(get_sentiments("bing"), relationship = "many-to-many") |>
  count(sentiment, word, sort = TRUE) |>
  group_by(sentiment) |>
  top_n(10) |>
  ungroup() |>
  mutate(word=reorder(word,n)) 


sent

```


## Sentiment Analysis

```{r}
out <- sent  |> 
  ggplot(mapping = aes(x = n, 
                       y = word, 
                       fill=sentiment)) +
  geom_col() +
  guides(fill = "none") + 
  facet_wrap(~ sentiment, 
             ncol = 1,
             scales = "free_y")

```


---

```{r}
#| fig.width: 15
#| fig.height: 5
print(out)
```

---

## Sentiment Analysis

```{r}
out <- full_text |> 
  inner_join(get_sentiments("bing"), relationship = "many-to-many") |>
  count(index = line %/% 2000, sentiment) |>
  pivot_wider(names_from = sentiment, 
              values_from = n, values_fill = 0) |> 
  mutate(sentiment = positive - negative) 

out

```

## Sentiment Analysis

```{r}
#| fig.height: 5
#| fig.width: 15
out |> 
  ggplot(mapping = aes(factor(index), sentiment)) +
  geom_col(show.legend = FALSE)
```

## Pronouns


```{r}
pronouns <- raw_text |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep=" ") |>
  filter(word1 %in% c("he", "she", "they")) |>
  filter(!word2 %in% stop_words$word, !str_detect(word2, "'")) |>
  count(word1, word2, sort=TRUE)

pronouns
```

## Pronouns

```{r}
out <- pronouns |> 
  group_by(word1) |> 
  top_n(15) |> 
  ggplot(mapping = aes(x=n, 
                       y=reorder(word2, n), 
                     fill=word1)) +
  geom_col() +
  guides(fill = "none") + 
  facet_wrap(~ word1, scales="free")
```

---

```{r}
#| fig.width: 8
#| fig.height: 8
print(out)
```

